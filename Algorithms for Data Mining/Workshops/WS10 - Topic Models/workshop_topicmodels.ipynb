{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science Workshop Week 8 - Topic Models\n",
    "\n",
    "The goal of this workshop is to extract topic models out of a corpus of 20 newsgroups. The corpus contains over 11000 documents. We will test Non-negative Matrix Factorization and Latent Dirichlet Allocation on this corpus with different pre-processing steps. \n",
    "\n",
    "In order to make the notebook work, you first have to install the textblob library. This can be simply be done by:\n",
    "- change to comand line and change in the anaconda2/bin directory\n",
    "- type: \"./pip install -U textblob\"\n",
    "- type: \"./python -m textblob.download_corpora\"\n",
    "\n",
    "or simply\n",
    "- !pip install textblob\n",
    "- !python -m textblob.download_corpora\n",
    "\n",
    "\n",
    "# Topic extraction with Non-negative Matrix Factorization and Latent Dirichlet Allocation\n",
    "\n",
    "\n",
    "The default parameters (n_samples / n_features / n_topics) should make\n",
    "the example runnable in a couple of tens of seconds. You can try to\n",
    "increase the dimensions of the problem, but be aware that the time\n",
    "complexity is polynomial in NMF. In LDA, the time complexity is\n",
    "proportional to (n_samples * iterations).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code to install textblob\n",
    "#!pip install textblob\n",
    "#!python -m textblob.download_corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named textblob",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-98e8e58a2c6d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecomposition\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mNMF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLatentDirichletAllocation\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtextblob\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTextBlob\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfetch_20newsgroups\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named textblob"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from time import time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from textblob import TextBlob\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "n_samples = 2000\n",
    "n_topics = 20\n",
    "n_top_words = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the 20 newsgroups dataset and vectorize it. We use a few heuristics\n",
    "# to filter out useless terms early on: the posts are stripped of headers,\n",
    "# footers and quoted replies, and common English words, words occurring in\n",
    "# only one document or in at least 95% of the documents are removed.\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "t0 = time()\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1,\n",
    "                             remove=('headers', 'footers', 'quotes'))\n",
    "data_samples = dataset.data[:n_samples]\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "# Look at the first entry\n",
    "data_samples[1]\n",
    "# How many documents do we have?\n",
    "len(data_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first step is data-preproccing. Write the function split_into_tokens that splits the documents into its single words. Subsequently, use the class CountVectorizer to transform the documents in the bag of words representation. How big is your resulting dictionary?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-4-c022002b0954>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-c022002b0954>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    split_into_tokens(data_samples[0])\u001b[0m\n\u001b[0m                    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "def split_into_tokens(message):\n",
    "    # put your code here\n",
    "\n",
    "# How does the first element look like?\n",
    "split_into_tokens(data_samples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bow_transformer = # put your code here\n",
    "# fit the bow transformer to the data\n",
    "data_bow = # get the transformed data\n",
    "\n",
    "print(len(bow_transformer.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your dictionary is probably pretty huge! We can restrict the number of words by using the min_df and the max_df parameters of the Countvectorizer class. min_df says we want to use words that occur in at least the given percentage of documents. max_df says we do not want to use documents that occur in more then max_df % of the documents as these are uninformative. Use min_df = 0.001 and max_df = 0.3 and retrain the CountVectorizer. Is your vocabulary smaller now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bow_transformer = # put your code here\n",
    "# fit the bow transformer to the data\n",
    "data_bow = # get the transformed data\n",
    "\n",
    "print(len(bow_transformer.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a non-negative matrix factorization (NMF) model\n",
    "\n",
    "The NMF can be trained by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nmf = NMF(n_components=n_topics, random_state=1, alpha=.1, l1_ratio=.5).fit(data_bow)\n",
    "nmf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can look at the topics. We first define a function that prints the top 20 words of all topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An then use it for our topic model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"\\nTopics in NMF model:\")\n",
    "feature_names = bow_transformer.get_feature_names()\n",
    "print_top_words(nmf, feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As we can see, the quality of our topics is quite poor. We have to do some more preprocessing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise 2: Stemming and lower case words\n",
    "\n",
    "Repeat the previous experiment for training an NMF, but this time, use stemming as preprocessing step. Also, convert all words to lower case. Did your vocabulary size decrease? Print the first element of data_samples in the Bag-of-word representation. Can you interpret this vector?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_into_lemmas(message):\n",
    "    # convert to lower case...\n",
    "    \n",
    "    # Do the stemming...\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create again the bow_transformer transform the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train NMF model and print the topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise 3: Pruning numbers and short words\n",
    "We still low quality topics, with numbers or only single letters. We want to prune this vocabulary. In your \n",
    "preprocessing step, remove all words that contain digits or that have less then 3 letters from the vocabulary.\n",
    "\n",
    "Hint:\n",
    "- you can query a string whether it contain a digit with .isalpha(). It returns true if the string does not contain a digit.\n",
    "- the following pattern might help you (Also see https://docs.python.org/3/tutorial/datastructures.html#list-comprehensions): \n",
    "        words = [word for word in words if *put your code here *]\n",
    "        \n",
    "How large is your vocabulary? Again train the topic model and print the topics. \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_into_lemmas(message):\n",
    "    # convert to lower case...\n",
    "    \n",
    "    # Do the stemming...\n",
    "   \n",
    "    # remove words that contain digits\n",
    "    \n",
    "    # remove words with len < 3\n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create new bow_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create NMF model and print topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise 4: TFIDF representation\n",
    "Repeat the training with the TFIDF representation instead of the Bag-of-words representation (see WS8). Can you see a difference in the topics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create tf_idf transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create NMF model using tf_idf data and print topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise 5: Removing stop words\n",
    "Our dictionary still contains many uninformative words. Download the stopload corpora from nltk. Use the english stop-words. In your data-preprocssing step, delete all words that are contained in the stop word. Hints:\n",
    "- download the corpora: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- get the stopwords:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Again use the function split_into_lemma for removing the stopwords. Do not use the build in function of the TFIDF vectorizer, as it does not work\n",
    "- You can check whether a word is contained in the list of stopwords with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'have'in stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the experiment and train the topic models. Do you get better topics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_into_lemmas(message):\n",
    "    # convert to lower case\n",
    "    \n",
    "    # Do the stemming...\n",
    "   \n",
    "    # remove words that contain digits\n",
    "    \n",
    "    # remove words with len < 3  \n",
    "    \n",
    "    # remove stop words \n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train new tf_idf transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train new NMF model and print the topics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
