{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science Workshop: Week 4\n",
    "\n",
    "This week we will use the theory about linear regression that we have learned last and this week. We will look at polynomial regression using the least squares solution derived before. Here, we will also see interesting findings about model-selection and see the over and under-fitting problem directly in our data set.\n",
    "\n",
    "Please read over the whole notebook. It contains several excercises (13) that you have to complete. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression in Matrix Form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='files/regression.png'>\n",
    "We want to find a linear function (line) that best fits the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Functions in vector form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More formally, we want to fit a linear function of the following form:\n",
    "    $$\\hat y = w_0 + \\sum_i w_i x_i $$\n",
    "$\\beta_0$ is the offset and $w_i$ defines the slope for the ith input.\n",
    "We can also write the output $\\hat{y}$ in vector form\n",
    "$$\\hat{y} =  \\boldsymbol x^T\\boldsymbol w \\textrm{ with } \\boldsymbol w = \\left[\\begin{array}{c}w_0 \\\\ \\vdots \\\\ w_d \\end{array} \\right] \\textrm{ and } \\boldsymbol x = \\left[\\begin{array}{c}1 \\\\ x_1 \\\\ \\vdots \\\\ x_d \\end{array} \\right]$$\n",
    "Note that we prepended a one to the $\\boldsymbol x$-vector which will multiply with the offset $w_0$ when computing the scalar product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrices for multiple outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now consider multiple samples $\\boldsymbol x_i$, where we will prepend again a $1$ to create the ${\\boldsymbol x}_{i} = \\left[\\begin{array}{c}1 \\\\ x_{i,1} \\\\ \\vdots \\\\ x_{i,d} \\end{array} \\right]$ \n",
    " vector. We can stack all ${\\boldsymbol x_{i}}$  in a matrix ${\\boldsymbol X} = \\left[\\begin{array}{c}{\\boldsymbol x}_{d}\\\\ \\vdots \\\\ {\\boldsymbol x}_{n}  \\end{array} \\right].$\n",
    "The output $\\hat y_i$ for each sample can also be  subsumed in a vector \n",
    "$\\hat{\\boldsymbol y} = \\left[\\begin{array}{c}\\hat{y}_{1}\\\\ \\vdots \\\\ \\hat{ y}_{n}  \\end{array} \\right] = \\left[\\begin{array}{c} {\\boldsymbol x_1}^T {\\boldsymbol w} \\\\ \\vdots \\\\ {\\boldsymbol x_n}^T {\\boldsymbol w}  \\end{array} \\right] = {\\boldsymbol X} {\\boldsymbol{w}}.$ Hence, the computation of all output values can be written as matrix vector product\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets do it in python..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will consider a 1-dimensional problem as illustrated below. We are given 10 training samples and we want to fit\n",
    "a line to these samples. Our line has 2 parameters, $\\beta_0$ and $\\beta_1$. Lets first look at the data and how we can compute a prediction using hand-picked \n",
    "$\\beta_0$ and $\\beta_1$ values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot\n",
    "\n",
    "data_train = pd.DataFrame.from_csv('regression_train.csv')\n",
    "data_test = pd.DataFrame.from_csv('regression_test.csv')\n",
    "\n",
    "data_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the training data as numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = data_train['x'].as_matrix()\n",
    "y_train = data_train['y'].as_matrix()\n",
    "\n",
    "x_test = data_test['x'].as_matrix()\n",
    "y_test = data_test['y'].as_matrix()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "## get input output vectors from the data frame and plot the data\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.clf()\n",
    "plt.plot(x_train,y_train, 'bo')\n",
    "plt.plot(x_test,y_test, 'g')\n",
    "plt.legend(('training points', 'ground truth'))\n",
    "plt.hold(True)\n",
    "plt.savefig('trainingdata.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data matrix\n",
    "As a first step, lets construct the $\\tilde{\\boldsymbol{X}}$ matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = np.column_stack((np.ones(x_train.shape), x_train))\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix calculus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to compute the derivatives of functions $f(\\boldsymbol{x})$ that take several input values, i.e., a vector $\\boldsymbol{x}$. As we have several input values $x_i$, we also need to derive the function with respect to each of them. The derivative of $f$ is therefore not a single value, but a vector that contains the derivative w.r.t each variable, i.e.\n",
    "$$\\frac{\\partial f(\\boldsymbol x)}{\\partial \\boldsymbol x} = \\left[\\frac{\\partial f(\\boldsymbol x)}{\\partial  x_1}, \\dots, \\frac{\\partial f(\\boldsymbol x)}{\\partial x_d}\\right].$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rules for the differentation are similar to the scalar rules, with small differences as listed below:\n",
    "* **Constants**:\n",
    "$$ \\frac{\\partial \\boldsymbol a}{\\partial \\boldsymbol x} = \\boldsymbol 0$$ \n",
    "* **Linear term:** \n",
    "$$ \\frac{\\partial \\boldsymbol A \\boldsymbol x}{\\partial \\boldsymbol x} = \\boldsymbol A$$\n",
    "* **Quadratic terms**:\n",
    "    $$ \\frac{\\partial \\boldsymbol x^T \\boldsymbol x}{\\partial \\boldsymbol x} = 2 \\boldsymbol x^T$$\n",
    "    \n",
    "    $$ \\frac{\\partial \\boldsymbol x^T \\boldsymbol A \\boldsymbol x}{\\partial \\boldsymbol x} = 2 \\boldsymbol x^T \\boldsymbol A,$$\n",
    "    if $\\boldsymbol A$ is a symmetrix matrix.\n",
    "* **Linearity**: Still holds...\n",
    "* **Chain Rule**: The chain rule is easy to generalize to the vector case.\n",
    "For a composition of functions, i.e., $y = \\boldsymbol f(\\boldsymbol g( \\boldsymbol x))$, we can again introduce an auxiliary variable $\\boldsymbol u = \\boldsymbol g( \\boldsymbol x)$. The derivative of  $y = f(\\boldsymbol g( \\boldsymbol x))$ is then given by\n",
    "$$ \\frac{\\partial f(\\boldsymbol g(\\boldsymbol x)) }{\\partial \\boldsymbol x} = \\frac{\\partial  f(\\boldsymbol u)}{\\partial \\boldsymbol u} \\frac{\\partial \\boldsymbol u}{\\partial \\boldsymbol x}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:** We want to compute the derivative of $$ (\\boldsymbol B \\boldsymbol x)^T(\\boldsymbol B \\boldsymbol x)$$. This function can be decomposed in $f(\\boldsymbol u) = \\boldsymbol u^T \\boldsymbol u$ and $\\boldsymbol u =\\boldsymbol g( \\boldsymbol x) = \\boldsymbol B \\boldsymbol x$, with $h(\\boldsymbol x) = f(\\boldsymbol g(\\boldsymbol x))$.\n",
    "* Compute derivative of $f$: $$ \\frac{\\partial  f(\\boldsymbol u)}{\\partial \\boldsymbol u} = \\frac{\\partial  (\\boldsymbol u^T \\boldsymbol u)}{\\partial \\boldsymbol u}= 2 \\boldsymbol u^T.$$\n",
    "* Compute derivative of $\\boldsymbol u$: $$ \\frac{\\partial  \\boldsymbol u}{\\partial \\boldsymbol x} = \\frac{\\partial  (\\boldsymbol B \\boldsymbol x)}{\\partial \\boldsymbol x} = \\boldsymbol B.$$\n",
    "* Final Result:\n",
    "$$ \\frac{\\partial f(\\boldsymbol g( \\boldsymbol x)) }{\\partial \\boldsymbol x} = \\frac{\\partial  f( \\boldsymbol u)}{\\partial \\boldsymbol u} \\frac{\\partial \\boldsymbol u}{\\partial \\boldsymbol x} = 2 \\boldsymbol u^T \\cdot \\boldsymbol B =  2 \\underbrace{\\boldsymbol x^T \\boldsymbol B^T}_{u^T} \\boldsymbol B$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the chain rule, compute the derivative of   \n",
    "$$E(\\boldsymbol x) = (\\boldsymbol a - 5\\boldsymbol x)^T \\boldsymbol A(\\boldsymbol{a} - 5 \\boldsymbol x).$$ Set the derivative to zero and compute the minimum. \n",
    "Plot $E(\\boldsymbol x)$ and $\\frac{\\partial E(\\boldsymbol x)}{\\partial x}$ as 3D plot. For $x_0$ and $x_1$, use an interval of $[-5, 5]$ for the plot using $51$ partitions for each dimension. Confirm your finding of the minimum in the plot. Note that $\\boldsymbol x$ is a 2x1 vector in this equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[1, 0.5], [0.5, 1]])\n",
    "a = np.array([[1], [0]])\n",
    "\n",
    "# specify data points for x0 and x1 (from - 5 to 5, using 51 uniformly distributed points)\n",
    "x0Array = np.linspace(-5, 5, 51)\n",
    "x1Array = np.linspace(-5, 5, 51)\n",
    "\n",
    "\n",
    "\n",
    "for i in range(0,50):\n",
    "    for j in range(0,50):\n",
    "        Earray[i,j] = # Put your code here...\n",
    "\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "\n",
    "x00Grid, x1Grid = np.meshgrid(x0Array, x1Array)\n",
    "\n",
    "# Plot the surface.\n",
    "surf = ax.plot_surface(x0Grid, x1Grid, Earray, cmap=cm.coolwarm,\n",
    "                       linewidth=0, antialiased=False)\n",
    "\n",
    "# Add a color bar which maps values to colors.\n",
    "fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "plt.xlabel('beta0')\n",
    "plt.ylabel('beta1')\n",
    "plt.savefig('errorfunction.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimizing the SSE function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to compute the minimum of the function, we need to compute its derivative and set it to zero.\n",
    "The SSE function is given by: $$SSE(\\boldsymbol{w}) = (\\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{w})^T  (\\boldsymbol{y} - {\\boldsymbol{X}}{\\boldsymbol{w}}).$$\n",
    "We can again use the chain rule and introduce the auxiliary variable $\\boldsymbol u = (\\boldsymbol{y} - {\\boldsymbol{X}}{\\boldsymbol{w}})$. The SSE is then given by \n",
    "$$SSE({\\boldsymbol w}) = f(\\boldsymbol g({\\boldsymbol w})),$$\n",
    "with $f(\\boldsymbol{u}) = \\boldsymbol{u}^T\\boldsymbol{u}$ and $\\boldsymbol{u} = \\boldsymbol{g}({ \\boldsymbol{w}})= (\\boldsymbol{y} - {\\boldsymbol{X}}{\\boldsymbol{w}}) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Compute derivative of $f$: $$ \\frac{\\partial  f(\\boldsymbol u)}{\\partial \\boldsymbol u} = \\frac{\\partial  (\\boldsymbol u^T \\boldsymbol u)}{\\partial \\boldsymbol u}= 2 \\boldsymbol u.$$\n",
    "* Compute derivative of $\\boldsymbol u$: $$ \\frac{\\partial  \\boldsymbol u}{\\partial \\boldsymbol{w}} = \\frac{\\partial (\\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{w}) }{\\partial  \\boldsymbol{w}} = - \\boldsymbol{X}^T.$$\n",
    "* Final Result:\n",
    "$$ \\frac{\\partial f(\\boldsymbol g( \\boldsymbol{w})) }{\\partial \\boldsymbol{w} } = \\frac{\\partial  f( \\boldsymbol u)}{\\partial \\boldsymbol u} \\frac{\\partial \\boldsymbol u}{\\partial \\boldsymbol{w}} = - 2 \\boldsymbol{X}^T \\boldsymbol u = - 2\\boldsymbol{X}^T(\\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{w}). $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving for the parameters\n",
    "\n",
    "We now need to set the derivative to $0$, i.e., \n",
    "$$\\frac{\\partial SSE( {\\boldsymbol{w}})}{\\partial {\\boldsymbol{w}}} = \\boldsymbol 0.$$\n",
    "Hence, the derivative w.r.t every dimension should be zero. Now, we solve for ${\\boldsymbol{w}}$ using the following steps:\n",
    "* Cancel constant factors:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$(\\boldsymbol{y} - {\\boldsymbol{X}}{\\boldsymbol{w}})^T {\\boldsymbol{X}} = \\boldsymbol 0. $$\n",
    "* Transpose on both sides (using $(\\boldsymbol{A} \\boldsymbol{B})^T = \\boldsymbol{B}^T \\boldsymbol{A}^T$): "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$${\\boldsymbol{X}}^T(\\boldsymbol{y} - {\\boldsymbol{X}}{\\boldsymbol{w}}) = \\boldsymbol 0 $$\n",
    "* Multiply out brackets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$${\\boldsymbol{X}}^T\\boldsymbol{y} - {\\boldsymbol{X}}^T {\\boldsymbol{X}}{\\boldsymbol{w}} = \\boldsymbol 0 $$\n",
    "* Bring ${\\boldsymbol{X}}^T\\boldsymbol{y}$ on other side"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$${\\boldsymbol{X}}^T {\\boldsymbol{X}}{\\boldsymbol{w}} = {\\boldsymbol{X}}^T\\boldsymbol{y}$$\n",
    "\n",
    "* Multiply both sides by the inverse of ${\\boldsymbol{X}}^T {\\boldsymbol{X}}$ (multiply on the left)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$${\\boldsymbol{w}} = ({\\boldsymbol{X}}^T {\\boldsymbol{X}})^{-1} {\\boldsymbol{X}}^T\\boldsymbol{y}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The least squares solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The least squares solution is given by:\n",
    "$${\\boldsymbol{w}} = ({\\boldsymbol{X}}^T {\\boldsymbol{X}})^{-1} {\\boldsymbol{X}}^T\\boldsymbol{y}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ** Note: ** The term $({\\boldsymbol{X}}^T{\\boldsymbol{X}})^{-1}{\\boldsymbol{X}}^T$ is called the pseudo inverse of matrix ${\\boldsymbol{X}}$. It is used to invert non-square matrices that could not be inverted otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 - Implementation in python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to implement the least squares solution that is given above in python. First, construct the Xtilde matrix.\n",
    "Subsequently implement the least squares solution. Try to avoud the linalg.inv method but use the numerically more stable linalg.solve \n",
    "method instead. \n",
    "* Plot again the training data, ground truth and prediction with the optimal least squares solution\n",
    "* What is the SSE of the optimal solution? \n",
    "* Compare your solution to the solution we have found in previous worshop with the grid search. How much do we \n",
    "do better now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check wether we still have Xtilde (otherwise we need to rerun all scripts)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.linalg as linalg\n",
    "\n",
    "w = # put your code here (you can also define additional variables if you want...)\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets plot our function\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "Xtest = np.column_stack((np.ones(x_test.shape), x_test))\n",
    "ytest_predicted = # Put your code here...\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x_test,y_test, 'g')\n",
    "plt.plot(x_test, ytest_predicted, 'r')\n",
    "plt.plot(x_train,y_train, 'bo')\n",
    "plt.legend(('training points', 'ground truth', 'prediction'), loc = 'lower right')\n",
    "plt.hold(True)\n",
    "plt.savefig('regression_LSS.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compute  the error on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = # Put your code here\n",
    "SSE = # Put your code here\n",
    "SSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In comparison to the hand-picked line parameters from above, our error is approximately half. However, we can still see that the approximation is far from perfect. The ground truth can not be represented by a line, so we will always have a rather large approximation error. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polynomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of fitting a line, we can also fit a polynomial. We want to fit $d$th order polynomials which are given by\n",
    "$$\\hat y = w_0 + \\sum_{i = 1}^d w_i x^i$$\n",
    "Note that, while $\\hat y$ is now non-linear in $x$, it is still linear in the parameters $w_i$. Hence, we can still apply linear regression here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Setting up the data matrix\n",
    " We can still describe $\\hat{y}$ as a scalar product, i.e.,\n",
    " $$\\hat{y} = w_0 + \\sum_{i = 1}^n w_i x^i = \\boldsymbol{x}^T\\boldsymbol{w}, \\textrm{ with } {\\boldsymbol{x}} = \\left[\\begin{array}{c} 1 \\\\ x^1 \\\\ x^2 \\\\ \\vdots \\\\ x^d  \\end{array}\\right] \\textrm{ and } {\\boldsymbol{w}} = \\left[\\begin{array}{c} w_0 \\\\ \\vdots \\\\ w_{d+1}  \\end{array}\\right]$$\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the data matrix in python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In python, we write a small function that does the feature expansion up to a certain degree for a given data set x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def getPolynomialDataMatrix(x, degree):\n",
    "    X = np.ones(x.shape)\n",
    "    for i in range(1,degree + 1):\n",
    "        X = np.column_stack((X, x ** i))\n",
    "    return X\n",
    "    \n",
    "print(getPolynomialDataMatrix(x_train, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Fit Polynomials with different degrees\n",
    "We now want to test different polynomials and see which one fits our data best. First, implement a function \n",
    "that computes the optimal beta values given the input data x, output data y and the desired degree of the polynomial. Reuse the getPolynomialDataMatrix given above in your function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.linalg as linalg\n",
    "\n",
    "def getWeightsForPolynomialFit(x,y,degree):\n",
    "    X = getPolynomialDataMatrix(x, degree)\n",
    " \n",
    "    w = # Put your code here\n",
    "\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given your getBetaForPolynomialFit function, plot the fitted function for a polynomial 1st, 2nd, 3rd and 4th degree.\n",
    "Can we now fit the structure of the function better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets plot our polynomials function\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x_test,y_test, 'g')\n",
    "plt.plot(x_train,y_train, 'bo')\n",
    "\n",
    "ytest1 = # Put your code for 1st degree predictions\n",
    "plt.plot(x_test, ytest1, 'r')\n",
    "\n",
    "ytest2 = # Put your code for 1st degree predictions\n",
    "plt.plot(x_test, ytest2, 'g')\n",
    "\n",
    "ytest3 = # Put your code for 1st degree predictions\n",
    "plt.plot(x_test, ytest3, 'm')\n",
    "\n",
    "ytest4 = # Put your code for 1st degree predictions\n",
    "plt.plot(x_test, ytest4, 'c')\n",
    "\n",
    "plt.legend(('training points', 'ground truth', '$x$', '$x^2$', '$x^3$', '$x^4$'), loc = 'lower right')\n",
    "\n",
    "plt.savefig('polynomial.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the predictions for 3rd and 4th order are already quite good. Lets see what happens if we increase \n",
    "the order of the polynomial. Repeat the same plots for for example 7, 10 and 12th order polynomials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets plot our polynomials function\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x_test,y_test, 'g')\n",
    "plt.plot(x_train,y_train, 'bo')\n",
    "\n",
    "\n",
    "ytest7 = # Put your code for 1st degree predictions\n",
    "plt.plot(x_test, ytest7, 'r')\n",
    "\n",
    "ytest10 = # Put your code for 1st degree predictions\n",
    "plt.plot(x_test, ytest10, 'c')\n",
    "\n",
    "ytest12 = # Put your code for 1st degree predictions\n",
    "plt.plot(x_test, ytest12, 'm')\n",
    "\n",
    "plt.ylim((-200, 200))\n",
    "plt.legend(('training points', 'ground truth', '$x^{7}$', '$x^{10}$', '$x^{12}$'), loc = 'lower right')\n",
    "\n",
    "plt.savefig('polynomial1.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see that the prediction performance of our polynomials degrade. Why is that? \n",
    "This effect is called overfitting. We have too many data dimensions and too little data in order to fit the polynomial. Overfitting can have 2 bad effects:\n",
    "* We fit the noise in the data\n",
    "* The function 'does what it wants' between the data points. It is underspecified what the function should do between the data points. \n",
    "\n",
    "However, for the training data, the fit is actually almost perfect. Thats another characteristic of overfitting!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Evaluating the Models\n",
    "We now want to evaluate the learned models to see which one works best. To do so, we compare polynomials of order\n",
    "1 to 12. Compute the squarred error on the training and on the test set for each of these polynomials and plot them \n",
    "as a function of the degree of the polynomial. Due to the huge differences in the error, use a log-scale plot for the y-axis (see pyplot.semilogy) What are your observations? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SSEtrain = np.zeros((11,1))\n",
    "SSEtest = np.zeros((11,1))\n",
    "\n",
    "\n",
    "# Feel free to use the functions getWeightsForPolynomialFit and getPolynomialDataMatrix\n",
    "for i in range(1,12):\n",
    "    \n",
    "    Xtrain = # Put your code here \n",
    "    Xtest = # Put your code here \n",
    "    \n",
    "    w = # Put your code here  \n",
    "    \n",
    "    SSEtrain[i - 1] = # Put your code here\n",
    "    SSEtest[i - 1] = # Put your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this...\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure();\n",
    "plt.semilogy(range(1,12), SSEtrain)\n",
    "plt.semilogy(range(1,12), SSEtest)\n",
    "plt.legend(('SSE on training set', 'SSE on test set'))\n",
    "plt.savefig('polynomial_evaluation.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting / Under-Fitting\n",
    "We can see that, while the training error decreases with a larger degree of the polynomial, the test set error\n",
    "significantly increases (which is the one we are interested in). This is a typical behavior we get for overfitting. \n",
    "We can make the following conclusions:\n",
    "* In order to fit the data well, we have to find the right model complexity\n",
    "* **Over-fitting:** The model-complexity is too high (degree > 4). We fit the noise and not the data\n",
    "* **Under-fitting:** The model-complexity is too low (degree < 3). We can not represent the data well enough.\n",
    "* For choosing the optimal model, we always have to consider the error on an independent test set not on the training set\n",
    "* On the training set, the error can be arbitrarily good. This is only an indication that the algorithm has learned the example by heart, not that it can generalize to new samples."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
